{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"About Me","text":"<p>Professionally, I am a software developer and entrepreneur with a keen interest in reducing complexity with over 30 years of professional experience.</p> <p>I am the founder of Omnigres, where we aspire to radically improve the way business systems are built using data-native architecture by turning Postgres into an application runtime.</p> <p>I have been involved in open source for a long time.</p> Which programming languages do I use? <p>These days, I write a lot of C/C++ and PL/pgSQL. I have a fairly good command of Rust. I ocassionally use OCaml and Prolog/Logtalk.</p> <p>I like languages that change or augment the way I think about programming.</p> <p>That being said, I know a lot of other languages (to varying degrees of competency), such as: JavaScript, Ruby, Zig, Java, Python, etc.</p> <p>In personal life, I am a husband and a father of one amazing daughter. I enjoy running, skiing, cycling and outdoors in general. Trying to figure out sailing.</p>"},{"location":"awesome/","title":"Awesome collection","text":"<p>You know all these \"awesome\" repositories that host a curated list of projects and materials pertaining to a particular topic (often, a programming language community)?</p> <p>Here's my cross-sectional \"awesome\" links collection. I was saving links in random places, but I kept losing them. So I am trying to persist them here.</p>"},{"location":"awesome/#programming-languages","title":"Programming languages","text":""},{"location":"awesome/#apl-inspired","title":"APL-inspired","text":"<ul> <li>BQN A modern take on APL?</li> <li>Uiua A stack-based array programming language.</li> <li>SaC Single-Assignment C is an array programming language predominantly suited for application areas such as numerically intensive applications and signal processing. Its distinctive feature is that it combines high-level program specifications with runtime efficiency similar to that of hand-optimized low-level specifications.</li> </ul>"},{"location":"awesome/#operating-systems","title":"Operating systems","text":"<ul> <li>Muen An x86/64 Separation Kernel for High Assurance. Written in Ada/SPARK.</li> </ul>"},{"location":"blog/","title":"Index","text":""},{"location":"blog/2023/02/16/what-happens-if-you-put-http-server-inside-postgres/","title":"What happens if you put HTTP server inside Postgres?","text":"<p>Benchmarks and performance claims are attention-grabbers, but that's not what drew me to work on Omnigres. When I first built a prototype of its HTTP server, I didn't foresee the desire to share the numbers. As we all know, getting benchmarks right is hard, and everybody's mileage may vary. But I'll show you some numbers here anyway. It'll be great to validate or invalidate my findings!</p> <p>Warning</p> <p>Since this article has been published, the API of Omnigres has been updated. I've not gotten to reflecting this in the aritcle yet. But the points made here are still valid!</p> <p>But first, what's Omnigres? The shortest definition I came up with is \"Postgres as a Platform.\"</p> <p>What do I mean by this? Well, this comes from the idea that much of your application and its infrastructure can live inside or next to Postgres. Your business logic, deployment orchestration, caching, job queues, API endpoints, form handlers, HTTP server, you name it. Instead of building a system composed of multiple services, you only need one server that takes care of it. And if it can scale, you've got something great!</p> <p>I am not the only one intrigued by this idea (and many don't like it, either): check out this HN thread.</p> <p>And it is not \"one-size-fits-all.\" But it fits a good set of problems. Anyway, that's a subject for another conversation.</p> <p>In the past weeks, I've taken the task of adding an embedded HTTP server to Omnigres. Since Omnigres is implemented in C, it was only natural for me to choose libh2o to implement the functionality of an HTTP server. It did help that H2O is known for its good performance characteristics.</p> <p>The idea was relatively simple: initialize a few HTTP worker processes, each quickly handing off details of the incoming HTTP requests to a Postgres instance in the same process.</p> <p>Sounds a bit weird, right? But bear with me, I will try to explain what's going on.</p> <p>Below is a sample piece of code of how an HTTP request can be handled.</p> <pre><code>select omni_httpd.http_response(headers =&gt; array[omni_httpd.http_header('content-type', 'text/html')], body =&gt; 'Hello, &lt;b&gt;' || users.name || '&lt;/b&gt;!'), 1 as priority\n       from request\n       inner join users on string_to_array(request.path,'/', '') = array[NULL, 'users', users.handle]\nunion\nselect omni_httpd.http_response(status =&gt; 404, body =&gt; json_build_object('method', request.method, 'path', request.path, 'query_string', request.query_string)), 0 AS priority\n       from request\norder by priority DESC\n</code></pre> <p>The first query in the union joins the request path of <code>/users/:user</code> with the <code>users</code> table on <code>users.handle</code> column and serves an HTML response with that user's name.</p> <p>The second query, being lower in priority, simply returns a 404 Not Found response with a JSON that contains some of the request details.</p> <pre><code>$ http localhost:9000/users/johndoe\nHTTP/1.1 200 OK\nConnection: keep-alive\nServer: omni_httpd-0.1\ncontent-type: text/html\ntransfer-encoding: chunked\n\nHello, &lt;b&gt;John&lt;/b&gt;!\n\n$ http POST localhost:9000/test\\?q\nHTTP/1.1 404 OK\nConnection: keep-alive\nServer: omni_httpd-0.1\ncontent-type: text/json\ntransfer-encoding: chunked\n\n{\n    \"method\": \"POST\",\n    \"path\": \"/test\",\n    \"query_string\": \"q\"\n}\n</code></pre> <p>Once all this worked, I decided to try benchmark it (mostly because I wanted to see how bad would it be):</p> <pre><code>$ wrk http://localhost:9000/users/johndoe -c 20 -t 10\nRunning 10s test @ http://localhost:9000/users/johndoe\n  10 threads and 20 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency   354.85us    1.40ms  47.24ms   97.05%\n    Req/Sec    12.51k     2.37k   25.01k    73.84%\n  1256199 requests in 10.10s, 179.70MB read\nRequests/sec: 124366.73\nTransfer/sec:     17.79MB\n</code></pre> <p>That's on MacBook Pro M1 Max. I've also ran the same test on an older x86_64-based Linux machine and got about 70K/sec.</p> <p>For reference, I decided to try comparing this with a similar (but quick-and-dirty-written) Node.js implementation that goes to the database for the same effect:</p> <pre><code>const http = require(\"http\");\n\nconst { Pool } = require('pg')\n\nconst pool = new Pool({ /* connection details */, max: 10, idleTimeMillis: 1000 * 60});\n\nconst host = 'localhost';\nconst port = 8000;\nconst requestListener = function (req, res) {\n    const split = req.url.split('/');\n    if (split.length == 3 &amp;&amp; split[1] == 'users') {\n      pool.query({name: \"q\", text: \"SELECT users.name FROM users WHERE users.handle = $1\", values: [split[2]]}).\n        then((qres) =&gt; {\n          if (qres.rows.length == 1) {\n            res.setHeader(\"Content-Type\", \"text/html\");\n            res.writeHead(200);\n            res.end(`Hello, &lt;b&gt;${qres.rows[0].name}&lt;/b&gt;!`);\n          } else {\n            res.setHeader(\"Content-Type\", \"application/json\");\n            res.writeHead(200);\n            res.end(JSON.stringify({method: req.method, path: req.url}));\n          }\n        })\n    } else {\n      res.setHeader(\"Content-Type\", \"application/json\");\n      res.writeHead(200);\n      res.end(JSON.stringify({method: req.method, path: req.url}));\n    }\n};\nconst server = http.createServer(requestListener);\nserver.listen(port, host, () =&gt; {\n    console.log(`Server is running on http://${host}:${port}`);\n});\n</code></pre> <p>And I re-ran the test:</p> <pre><code>$ wrk http://localhost:8000/users/johndoe -c 20 -t 10\nRunning 10s test @ http://localhost:8000/users/johndoe\n  10 threads and 20 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     1.11ms    0.99ms  33.09ms   99.15%\n    Req/Sec     1.89k   166.30     2.19k    92.18%\n  190319 requests in 10.10s, 33.76MB read\nRequests/sec:  18838.56\nTransfer/sec:      3.34MB\n</code></pre> <p>From the surface, it looks like omni_httpd is faster for the time being! I am sure with time it'll get slower and I did not explore enough optimizations available for the Node.js experiment (please contribute to make this picture fairer!). But it shows what bundling can do to performance!</p> <p>Update: As someone pointed out, using TCP locally brings unnecessary overhead where UNIX sockets would have performed better. So, I re-ran the Node.js test over UNIX sockets:</p> <pre><code>$ wrk http://localhost:8000/users/johndoe -c 20 -t 10\nRunning 10s test @ http://localhost:8000/users/johndoe\n  10 threads and 20 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency   663.15us  224.77us   9.87ms   93.24%\n    Req/Sec     3.05k   139.17     3.30k    88.71%\n  306649 requests in 10.10s, 54.39MB read\nRequests/sec:  30361.88\nTransfer/sec:      5.39MB\n</code></pre> <p>It's indeed quite a bit faster but still way below the numbers I was able to get with <code>omni_httpd</code>.</p> <p>I want to emphasize that performance was not and is not a primary goal behind this work. What I think is more important there is changing the way we think about serving HTTP clients, doing data-aware routing, etc. There's still a lot of research to be done on this end.</p> <p>I also want to mention that this is still early work. There's a lot to be done to make this server production-grade (starting from enabling HTTPS support, and improving the architecture to make HTTP2 multiplexing really work, but not ending there), as well as building out other extensions to make building applications really easy and convenient.</p> <p>If you want to chat more, join us on Discord or Telegram!</p>"},{"location":"blog/2023/07/20/omnigres-developer-experience/","title":"Omnigres Developer Experience","text":"<p>Software developer's job is not an easy one; anything that makes it less frustrating and makes developers more productive is highly sought for. This is why the most successful developer tools are usually the ones that have an amazing developer experience.</p> <p>Omnigres turns Postgres into a complete Application Platform, and by doing that, we must focus on Development, Debugging and Deployment Experience (D3X) as the #1 priority.</p> <p>Mature databases, such as Postgres, come with a very specific, database-centric experience. It can be attributed to the fact that we are all used to having database workflows distinct from our development activities (because they are considered to be separate things), inertia and tradition.</p> <p>One of the core idea in Omnigres is that your database can run your application, too. There are many reasons for that (performance, simpler and cheaper deployment, atomic migrations, etc.). What's important is that this means we have to make the development experience of this approach familiar, smooth and, dare I say, a bit magical.</p> <p>Omnigres is still a young project. It has already contributed to an improved experience, but let's peak into the future and see a more complete picture of where it's going.</p>"},{"location":"blog/2023/07/20/omnigres-developer-experience/#where-do-we-start","title":"Where do we start?","text":"<p>Every project has to begin somewhere. We're taking cues from most popular tools. It should be possible to create and run projects as simply as running the following commands:</p> <pre><code>$ omnigres init\n$ omnigres run\n</code></pre> <p>That's it, at this point you have an application that you can access over HTTP. Omnigres has its own HTTP server built in with WebSocket support.</p> <p>This tool will also handle provisioning and running Postgres installations under the hood. Simply change the desired version in the generated config file, and you're running a version you need.</p>"},{"location":"blog/2023/07/20/omnigres-developer-experience/#managing-schema-migrations","title":"Managing schema migrations","text":"<p>You don't really need an external tool to run incremental migrations, your database is perfectly capable of doing that. omni_schema loads all your migration files in the correct order. Similar to many projects, simply put an order-prefixed (<code>1_create_users.sql</code>, <code>2_add_deleted_at_to_users.sql</code> or a timestamp if you prefer) SQL files into a <code>migrations</code> directory and the tooling will pick those up to run the migrations.</p> <p>And for cases where incremental migrations can be unambiguously deduced from a DDL (with hints or without), it can make your life even easier: simply edit your <code>create table</code> statement in place.</p>"},{"location":"blog/2023/07/20/omnigres-developer-experience/#putting-functionality-into-postgres","title":"Putting functionality into Postgres","text":"<p>One of the reasons people avoid putting functions inside of Postgres is that the very experience of doing so can be frustrating. Should they be part of incremental migrations to ensure they are used with the correct data model? Should they be deployed separately?</p> <p>Since we don't really need to change functions incrementally, functions can be simply reloaded from a single source of truth. That's what <code>omni_schema</code> already does, and not just for functions. It simply gets the new definitions override the old ones as part of the routine migration process.</p>"},{"location":"blog/2023/07/20/omnigres-developer-experience/#bring-your-own-language","title":"Bring Your Own Language","text":"<p>Postgres already supports a few languages one can write functions in (PL/pgSQL, Python, Perl, Rust, JavaScript, Java, Tcl) and more are coming <sup>1</sup>.</p> <p>But you know what sucks the most today when you have to write a function in one of those languages? You have to stick it inside of SQL and your editors are mostly not very helpful after that, as this is no longer a [Python | JavaScript | Rust] file.</p> <pre><code>create function pymax(a integer, b integer)\n  returns integer\nas $$\n  if (a is None) or (b is None):\n    return None\n  if a &gt; b:\n    return a\n  return b\n$$ language plpython3;\n</code></pre> <p>However, the tooling can simply find your <code>.py</code>, <code>.js</code> (or other language) files and create SQL functions out of them.</p>"},{"location":"blog/2023/07/20/omnigres-developer-experience/#how-do-i-develop-my-web-applications-end-to-end","title":"How do I develop my web applications end-to-end?","text":"<p>Without prescribing a single best approach (there probably isn't!), Omnigres offers a few components and approaches:</p> <ul> <li>HTML templates</li> <li>REST/GraphQL integration</li> <li>Over-the-wire components (similar to Phoenix LiveVew)</li> <li>Integrated UI framework (SQLPage is a source of inspiration)  </li> </ul> <p>As with functions, all of these are developed in regular files so that the experience is familiar and convenient.</p>"},{"location":"blog/2023/07/20/omnigres-developer-experience/#how-do-my-files-get-to-deployment","title":"How do my files get to deployment?","text":"<p>Aha, that's a great question! </p> <p>We use a virtual file system extension, omni_vfs and <code>omni_git</code> to take your files with you. Once you are ready to deploy you make your database do a Git pull (<code>select omni_git.pull(...)</code>) and the files are getting accessed using a Git VFS in production as opposed to local VFS when you are developing.</p> <p>And this doesn't only apply to migration-related files. Your templates, static assets, all of that can be retrieved this way.</p>"},{"location":"blog/2023/07/20/omnigres-developer-experience/#augmenting-postgres","title":"Augmenting Postgres","text":"<p>Arguably, one of the most exciting things about Postgres is its ecosystem of extensions that keeps growing. From geo-informational systems and time series to machine learning!</p> <p>However, installing extensions easily and reliably across platforms is something that stops a lot of people as extensions maybe non-trivial to build (they have external requirements) and managing that both for development (everybody's machine and environment is subtly different) and production can be frustrating.</p> <p>It really should be as simple as doing something like this:</p> <pre><code>$ pgpm add vector\n</code></pre> <p>and having it stored in your config, downloaded/built for your local development experience and automatically rolled out when deployed. We're taking a lot of cues from Rust's cargo here.</p> <p>As opposed to some other approaches, pgpm<sup>2</sup> is focused on configuring, building and packaging natively, without relying on isolated environments (such as Docker) so that you can run the extensions without having Postgres contained. It's an expert configuration system, if you will.</p>"},{"location":"blog/2023/07/20/omnigres-developer-experience/#reactive-queries","title":"Reactive Queries","text":"<p>Data is often treated as inert matter. We write it down and until we query against it, it just sits there.</p> <p>But the reality we're building applications for is complicated. We want data to have effects outside of a single transaction's scope. For example, what if we wanted to notify inactive users or run an onboarding campaign that is tailored to what the user is doing and their patterns?</p> <p>The idea behind reactive queries is that you can define what must happen should certain condition occur at some point. It's kind of like triggers but for sets of conditions as opposed to being bound to a particular entity.</p>"},{"location":"blog/2023/07/20/omnigres-developer-experience/#job-queueing","title":"Job queueing","text":"<p>There are things we shouldn't do while handling a request, especially when they take time and we don't need an immediate output. So, instead of having to manage an external job server (and maybe even Redis for it?) Omnigres has an embedded job server that uses local and remote workers to complete these. Jobs benefit from being close to data and can be trigerred by reactive queries, too.</p>"},{"location":"blog/2023/07/20/omnigres-developer-experience/#scaling","title":"Scaling","text":"<p>Even though your need in extra computing goes down when performance is higher, you will need to scale at some point. Building on and augmenting Postgres' own replication facilities, Omnigres can grow your deployment smoothly. Having control over migrations workflow gives us  better control over scaling roll out and schema synchronization.</p> <p>Beyond physical and logical replication, novel approaches like Neon DB can also facilitate beter scaling and elastic resource use.</p> <p>Think about it this way: Omnigres is ultimately an application server with a database inside. That database's replication, foreign data wrappers and elastic provisioning allows the application server to scale horizontally.</p>"},{"location":"blog/2023/07/20/omnigres-developer-experience/#what-do-i-do-with-legacy-pieces","title":"What do I do with legacy pieces?","text":"<p>Don't throw them out! Also, you don't have to rewrite them right away, either. Omnigres platform can also be used as a manager/orchestrator for external components (like containers<sup>3</sup>) so that you maintain the source of truth in a single place and can query against or sent traffic to these components based on the data you have.</p>"},{"location":"blog/2023/07/20/omnigres-developer-experience/#where-are-we-at","title":"Where are we at?","text":"<p>Some of the described functionality is already there, some is the works, some are being researched and others are just a vision at the moment. You can find a bit more progress clarity on this roadmap.</p> <ol> <li> <p>I've recently started work on omni_prolog. How about some expert systems inside Postgres?\u00a0\u21a9</p> </li> <li> <p>Postgres Package Manager, currently a work-in-progress\u00a0\u21a9</p> </li> <li> <p>There's omni_containers extension already, but it's not quite documented yet.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2023/04/04/make-postgres-an-application-server-gamified/","title":"Make Postgres an Application Server, Gamified","text":"<p>Have you ever wondered if Postgres can be a fully self-sufficient platform for your application? Learn how to make it become an application server and win some prizes, too!</p>"},{"location":"blog/2023/04/04/make-postgres-an-application-server-gamified/#intro","title":"Intro","text":"<p>Omnigres is a project with the goal of making Postgres a complete platform for developing and deploying applications. It enables colocating your business logic with the data and exploit the benefits of such an approach. It's very early but it is already showing promise and is ready for some early adopters!</p> <p>The focus of this first, informal contest is to explore its HTTP server capabilities. With omni_httpd one can serve HTTP requests using SQL queries.</p> <p>What's the deadline?</p> <p>It will end May 1, 2023 unless extended further.</p>"},{"location":"blog/2023/04/04/make-postgres-an-application-server-gamified/#brief","title":"Brief","text":"<p>In its documentation there is an example MOTD service that is barely scratching the surface of what's possible. This is where we start. You can find the copy of the code below, too.</p> MOTD service code <pre><code>-- MOTDs stored here\ncreate table if not exists motd\n(\n    id        int primary key generated always as identity,\n    content   text,\n    posted_at timestamp default now()\n);\n\n-- Shows last MOTD\ncreate or replace function show_motd() returns setof omni_httpd.http_response as\n$$\nselect\n    omni_httpd.http_response('Posted at ' || posted_at || E'\\n' || content)\nfrom\n    motd\norder by\n    posted_at desc\nlimit 1;\n$$ language sql;\n\n-- Shows when there are no MOTDs\ncreate or replace function no_motd() returns setof omni_httpd.http_response as\n$$\nselect omni_httpd.http_response('No MOTD');\n$$\n    language sql;\n\n-- Creates a new MOTD \ncreate or replace function update_motd(request omni_httpd.http_request) returns omni_httpd.http_response as\n$$\ninsert\ninto\n    motd (content)\nvalues\n    (convert_from(request.body, 'UTF8'))\nreturning omni_httpd.http_response(status =&gt; 201);\n$$\n    language sql;\n\n-- Handlers\nupdate omni_httpd.handlers\nset\n    query = (select\n                 omni_httpd.cascading_query(name, query order by priority desc nulls last)\n             from\n                 (values\n                      -- GET\n                      ('show', $$select show_motd() from request where request.method = 'GET'$$, 1),\n                      -- POST\n                      ('update', $$select update_motd(request.*) from request where request.method = 'POST'$$, 1),\n                      -- No MOTDs\n                      ('fallback', $$select no_motd() from request where request.method = 'GET'$$,\n                       0)) handlers(name, query, priority));\n</code></pre> <p>Your objective is to solve one or more of the following challenges:</p>"},{"location":"blog/2023/04/04/make-postgres-an-application-server-gamified/#challenges","title":"Challenges","text":"<p>Tip</p> <p>Be the first one to solve either challenge to win a prize.</p>"},{"location":"blog/2023/04/04/make-postgres-an-application-server-gamified/#1-make-it-serve-html-and-json","title":"#1: Make it Serve HTML and JSON","text":"<p>Depending on the <code>Accept</code> header (and/or query path/string), make the service render HTML or JSON for a MOTD.</p> <p>For example:</p> <pre><code>$ curl --header \"Accept: application/json\" http://localhost:8080\n{\"content\": \"...\", \"posted_at\": \"...\"}\n</code></pre> First prize claimed <p>@ggaughan solved it first. You can still get the Finisher's prize.</p>"},{"location":"blog/2023/04/04/make-postgres-an-application-server-gamified/#2-authorized-user-updates","title":"#2: Authorized User Updates","text":"<p>Make it possible to update MOTD only by authorized users. It's up to you how you define \"authorized\" but be reasonable!</p> First prize claimed <p>@kartikynwa solved it first. You can still get the Finisher's prize.</p>"},{"location":"blog/2023/04/04/make-postgres-an-application-server-gamified/#3-separate-rooms","title":"#3: Separate Rooms","text":"<p>Instead of having one global MOTD, allow updating MOTD by \"rooms\" (room name can be derived from the path or the query string).</p> <p>For example: </p> <pre><code>POST /omnigres \"Check out Omnigres\" # =&gt; HTTP/1.1 201 OK\nPOST /postgres \"We're all waiting for Postgres 16\" # =&gt; HTTP/1.1 201 OK\n\nGET /postgres # =&gt; HTTP/1.1 200 OK\nPosted at 2023-04-04 08:01:23:13.617115\nWe're all waiting for Postgres 16\n\nGET /postgres # =&gt; HTTP/1.1 200 OK\nPosted at 2023-04-04 08:01:23:13.317115\nCheck out Omnigres\n</code></pre> First prize claimed <p>@ggaughan solved it first. You can still get the Finisher's prize.</p>"},{"location":"blog/2023/04/04/make-postgres-an-application-server-gamified/#surprise-challenge","title":"Surprise Challenge","text":"<p>Build something not listed in the above challenges and make it awesome. First three entries win!</p>"},{"location":"blog/2023/04/04/make-postgres-an-application-server-gamified/#where-to-learn","title":"Where to Learn?","text":"<p>The easiest way to start Omnigres is to use a container image:</p> Default imageSlim image <p> The image below is rather large (over 8Gb). If you prefer a smaller one, select the next tab.</p> <pre><code>docker volume create omnigres\ndocker run --name omnigres -e POSTGRES_PASSWORD=omnigres -e POSTGRES_USER=omnigres \\\n                            -e POSTGRES_DB=omnigres --mount source=omnigres,target=/var/lib/postgresql/data \\\n            -p 5432:5432 -p 8080:8080 --rm ghcr.io/omnigres/omnigres:latest\n# Now you can connect to it:\npsql -h localhost -p 5432 -U omnigres omnigres # password is `omnigres`\n</code></pre> <pre><code>docker volume create omnigres\ndocker run --name omnigres -e POSTGRES_PASSWORD=omnigres -e POSTGRES_USER=omnigres \\\n                           -e POSTGRES_DB=omnigres --mount source=omnigres,target=/var/lib/postgresql/data \\\n           -p 5432:5432 -p 8080:8080 --rm ghcr.io/omnigres/omnigres-slim:latest\n# Now you can connect to it:\npsql -h localhost -p 5432 -U omnigres omnigres # password is `omnigres`\n</code></pre> <p>Please refer to Omnigres documentation or drop by our Discord server to ask questions.</p>"},{"location":"blog/2023/04/04/make-postgres-an-application-server-gamified/#prizes","title":"Prizes","text":""},{"location":"blog/2023/04/04/make-postgres-an-application-server-gamified/#first","title":"First","text":"<p>Solving any challenge first gives you a prize of $30 USD or a comparable equivalent. It also includes the Finisher's prize.</p>"},{"location":"blog/2023/04/04/make-postgres-an-application-server-gamified/#finisher","title":"Finisher","text":"<p>Solving any challenge gives you a shout out in the post, Twitter and other media. Your name will be documented in Omnigres documentation where we'll record this contest for posterity.</p>"},{"location":"blog/2023/04/04/make-postgres-an-application-server-gamified/#surprise","title":"Surprise","text":"<p>You built something not listed in the above challenges and it is awesome? There are three $50 prizes for this, one for each entry! Includes the Finisher's prize.</p>"},{"location":"blog/2023/04/04/make-postgres-an-application-server-gamified/#solution-submissions","title":"Solution Submissions","text":"<p>Please post your solution somewhere like Github Gist and post in a comment below this post.</p>"},{"location":"blog/2023/04/10/avoiding-postgres-extensions-limitations/","title":"Avoiding Postgres Extensions Limitations","text":"<p>Postgres extensions are great. They enable fantastic use cases and bring new capabilities to one of the most loved open source databases. But there are edges in some of its features and this can be heard in conversations: limitations of the upgrade system, lack of parameterized initialization, search path/OID resolution issues, hard-wired dependency on <code>.control</code> files, schema droppage <sup>1</sup>, etc.</p> <p>However, the beauty of it is that what we ultimately want from extensions does not need to use <code>CREATE EXTENSION</code>'s framework.</p> <p>I've seen it a few times where newcomers to the extension world of Postgres learn to understand the relationship between <code>.control</code>, <code>.sql</code> and <code>.so</code> files. The questions I keep hearing are something like this: </p> <ul> <li>are extensions required to have an .so file?</li> <li>are SQL files required?</li> <li>where do we make users store metadata so that it doesn't get dropped accidentally?</li> </ul> <p>It's convenient to think about Postgres extensions feature as a whole, from <code>CREATE EXTENSION</code> down to your functions. However, I think it hides the fact that technically speaking, extension are a relatively thin mechanism on top of some of the more fundamental capabilities that Postgres provides.</p> <ul> <li>It allows one to execute SQL (duh!)</li> <li>It allows to define functions that are contained in an <code>.so</code> file<sup>2</sup>.</li> </ul> <p>Postgres' vanilla extension framework reads the <code>.control</code> file, executes SQL scripts with a bit of quick-and-dirty string value replacements (like <code>MODULE_PATHNAME</code> or <code>@extschema@</code>) and these scripts deal with provisioning all that extension requires.</p> <p>You can see most of this code in src/backend/commands/extension.c. You can see that that code doesn't really deal with <code>.so</code> files. That code is actually in src/backend/utils/fmgr/dfmgr.c, like <code>load_external_function</code>.</p> <p>What this means is that you don't really need to follow the path charted by the framework to add that extended functionality to your database. You can roll your own upgrades, you can can have multiple <code>.so</code> files (instead of depending on <code>MODULE_PATHNAME</code>), you can call whatever initialization callbacks you want during installation, you don't need <code>.control</code> files. You can think of a lot of your own cases.</p> <p>What do you have to give up for this? You have to give up <code>CREATE EXTENSION</code>. Instead, you'd need to do something like this:</p> <pre><code>psql=# select myextmgr.install('extension_name');\n</code></pre> <p>Which is probably not a big ask (implementing the <code>install</code> function itself is a bigger one!). In fact, it may be even more interesting because one can use this function over an entire dataset. For example, this will allow one to install multiple extensions not known ahead of time.</p> <p>But...</p> <p>How would we call <code>create function</code> and provide the path to <code>.so</code> files if it is not known ahead of time?</p> <p>Great question. There are two ways I can see, depending on how deep you want to go:</p> <ol> <li>You can <code>format()</code>    your <code>create function</code> query to supply the correct path with <code>%L</code>. I personally don't love it, but it's done a lot in PL/pgSQL.</li> <li>If you're writing this in C, there's <code>ProcedureCreate</code>.    It has an ungodly number of parameters, but once you're through, it works really well!</li> </ol> <p>A bigger thing you're potentially giving up here is that if you're developing an extension that you want others to use and it can't be fit into the mold provided by vanilla Postgres extensions framework, well, you're presented with a new challenge. You will need to find a way to convince your users to use your installation method. Whether it is worth it entirely depends on whether the limitations you're concerned about are worth overcoming.</p> <ol> <li> <p>Unless explicitly depended on afterwards\u00a0\u21a9</p> </li> <li> <p>Filename extension that means shared object.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2023/04/23/structured-postgres-regression-tests/","title":"Structured Postgres Regression Tests","text":"<p>I've been using <code>pg_regress</code> tests for a while. It's generally a great way to ensure the behavior of your Postgres code works and continues working as expected. However, as my tests became larger, I started getting lost in them; and there are limits as to what you can test by having a <code>psql</code> session.</p> <p>If you don't know, <code>pg_regress</code> basically takes an SQL file you give to it and sends it to <code>psql</code> running against a Postgres instance (either managed by <code>pg_regress</code> or by you) and compares the output against a previous stored execution log. If there are no differences, all is good (aka \"tests passed\".)</p> <p>But I found that at least in my practice, tests tend to become rather large and it's hard to find separation between steps, individual tests and so on. Of course, you can put every individual test into a separate file, but that feels like a bandaid.</p> <p>So I thought, \"how can I structure this better? There must be a way.\"</p> <p>My hunch was that if I can put both inputs (queries) and outputs (results, errors, etc.) into a machine-processable format that is also very visual, I can do a lot with it. I didn't want to write JSON, though. It doesn't force you into having a visual structure.</p> <p>Unsurprisingly, YAML fit the bill. As much as I may not be its fan (it has design flaws and is definitely being overused for, uhm, \"programming\"), but it does provide a good visual structure, can be essentially queried and has some interesting features like tags and anchors. And, importantly, it has pretty good support for multiline strings<sup>1</sup>!</p> <p>So, instead of having something like this in your test:</p> <pre><code>select true as value\n value\n-------\n    t\n(1 row)    \n</code></pre> <p>...what if you can have something a bit more structured?</p> <pre><code>query: select true as value\nresults:\n- value: t\n</code></pre> <p>At this point I got excited and decided that there's no going back and <code>pg_yregress</code> was prototyped.</p> <p>By focusing on the structure one can feed into it, one can provide a lot of information to it, such as:</p> <ul> <li>configuration of the instances to be tested against</li> <li>initialization sequences</li> <li>reusable queries (hello, YAML aliases!)</li> <li>sending tests to multiple instances (want to test a replication scenario?)<sup>2</sup></li> </ul> <p>Unlike <code>pg_regress</code>, it doesn't use <code>psql</code> and this opens some interesting opportunities. For example, it can be used to test binary encodings as it simply uses <code>libpq</code>:</p> <pre><code>query: select true as value\nbinary: true\nresults:\n- value: 0x01\n</code></pre> <p>Also, it wraps queries into transactions by default<sup>3</sup>, which removes a lot of <code>begin</code>/<code>rollback</code> noise in <code>pg_regress</code>-based tests.</p> <p>It's still pretty new and rough around edges but I am already migrating Omnigres to it and will continue adding necessary features and improving the user experience. Check out the documentation if you are interested.</p> <p>Feedback, suggestions and contributions will be appreciated, so don't be shy!</p> <ol> <li> <p>not a lot of queries easily fit into a single line, so...\u00a0\u21a9</p> </li> <li> <p>Not all features are ready, please be patient or consider contributing. I'll get to it if you don't!\u00a0\u21a9</p> </li> <li> <p>Actually, there's no way to turn this behavior off right now, but it'll come soon as it is a trivial change.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2025/02/26/postgres-extensions-day-2025-montreal-registration-and-call-for-speakers/","title":"Postgres Extensions Day 2025 Montreal: Registration and Call for Speakers","text":"<p>On May 12th, 2025, Postgres Extension Developers Coalition (PGEDC) will host Postgres Extensions Day 2025 in Montreal, QC.</p> <p>It is a one-day, in-person event dedicated to the comprehensive exploration of Postgres extension \u2013 encompassing development, ecosystem, and operational concerns.</p> <p>The program includes in-depth technical presentations, practical demonstrations, and structured discussions designed to foster collaboration and innovation. Whether you are new to Postgres extensions or an experienced developer, you will gain valuable insights, advanced techniques, and inspiration for your future work.</p> <p>This free, community-led event operates independently and is unaffiliated with other events. Prior registration is required.</p> <p>Call for Speakers is open until April 1st.</p> <p>Topics of interest include:</p> <ul> <li>Teaching extension development</li> <li>Extension packaging &amp; delivery</li> <li>Libraries and frameworks for extensions</li> <li>Gaps and challenges</li> <li>Implementing extensions in different languages</li> <li>Interesting techniques &amp; discoveries</li> <li>Operating extensions in production</li> <li>Extensions security</li> </ul> <p>If the topic above is not listed, please still feel free to submit your talk.</p> <p>Register and submit your talks soon!</p>"},{"location":"blog/2023/04/05/plrust-just-shipped-easy-way-to-try-it-out/","title":"PL/Rust Just Shipped: Easy Way to Try It Out","text":"<p>As a Rust enthusiast and a contributor to a sister project I am stoked about the release of PL/Rust 1.0.0 that was just announced.</p> <p>However, its setup instructions are rather long and it takes time to build it. So I took the time to prepare a build for you to try.</p> <p>As Omnigres is intended to be an application platform, support for multiple languages is important. Hence I decided to spend the day making sure PL/Rust is shipped with Omnigres. Omnigres container image is simply a Postgres image with Omnigres extensions (and now PL/Rust, too.) provisioned.</p>"},{"location":"blog/2023/04/05/plrust-just-shipped-easy-way-to-try-it-out/#try-it-out-now","title":"Try it out now","text":"<p>You can start Omnigres with the following commands:</p> <pre><code>docker volume create omnigres\ndocker run --name omnigres -e POSTGRES_PASSWORD=omnigres -e POSTGRES_USER=omnigres \\\n                           -e POSTGRES_DB=omnigres --mount source=omnigres,target=/var/lib/postgresql/data \\\n           -p 5432:5432 -p 8080:8080 --rm ghcr.io/omnigres/omnigres:latest\n# Now you can connect to it:\npsql -h localhost -p 5432 -U omnigres omnigres # password is `omnigres`\n</code></pre> <p>That's it, now you can try this:</p> <pre><code>create extension plrust;\n\ncreate function test() returns bool language plrust as $$ Ok(Some(true)) $$;\n\nselect test(); -- =&gt; t\n</code></pre> <p>It works! The sky is the limit now  </p>"},{"location":"blog/2023/04/05/plrust-just-shipped-easy-way-to-try-it-out/#what-have-i-learned","title":"What Have I Learned?","text":"<p>I got up at 5AM today to make this happen. I am writing this past 5PM. It's been a long day, and I've made a lot of mistakes on the way, and figured out some gotchas.</p> <p>The amount of space PL/Rust takes is rather not insignificant. I've measured about an 8Gb increase of the image size. While it is not the end of the world for standard cases, this is something to be aware of <sup>1</sup>. I've been told that there are some ideas on how to improve the space usage but it'll never be really slim (Rust compiler itself is not small.)</p> <p>If you're building PL/Rust in environments like containers, make sure <code>USER</code> environment variable is set. It'll fail to build if it is not.</p> <p>It also seems to be necessary to have at least 16GB RAM to build it. Not sure about memory requirements in runtime just yet.</p> <p>First-time function compilation takes an awfully long time. However, one can prime it ahead of time by compiling a test function in a throwaway database. That's what I did in the image above.</p> <p>It took a while to make sure the container works exactly how I want it to, but it was worth it!</p> <ol> <li> <p>For those not needing the bulk (and Rust), I've added the <code>omnigres-slim</code> version of the image.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2023/04/09/sum-types-in-postgres/","title":"Sum Types in Postgres","text":"<p>At times, representing the variety of types a value can take through multiple tables can taxing, both in terms of development complexity as well as, potentially, performance. You must just need a value to be of any of the given variants. I've set out to build a generalized mechanism for defining these. </p> <p>Can you give me an example?</p> <p>Sure. Let's say we want to store geometric primitives<sup>1</sup> and be able to find certain shapes with certain properties.</p> <p>One can put different shapes into different tables or even use multiple columns per shape type. But this makes the schema fairly complicated.</p> <p>Wouldn't it be cool to just store any shape in a single column?</p>"},{"location":"blog/2023/04/09/sum-types-in-postgres/#intro","title":"Intro","text":"<p>I am working on making Postgres a full-featured application development platform, so this issue came up a few times. For example, I want to be able to return different kinds of responses to HTTP requests, like upgrading to WebSockets, delaying a response, terminating a connection, etc. The <code>http_response</code> type is limited to conventional responses.</p> <p>So I thought, is it possible to support this type of thing in Postgres? I develop a number of extensions and have used its C API surface extensively. I was pretty sure this can be achieved! A few days later, the support has materialized.</p> <p>The basic idea is simple: </p> <ul> <li>Register a base type for every sum type and list all variant types OIDs in extension's configuration</li> <li>If all variants are fixed size, sum type is also fixed size of the largest variant (plus discriminant)</li> <li>Use underlying variant type's I/O functions</li> <li>Dispatch I/O and conversion functions to generalized implementations provided by the extension<sup>2</sup></li> </ul> <p>It's been a pretty fun experience connecting all the dots and using C API to define types, functions, casts, etc.</p> <p>The way it works is pretty simple:</p> <pre><code>omni_types=# select omni_types.sum_type('geom', 'point', 'circle');\nsum_type\n----------\n geom\n(1 row)\n</code></pre> <p>Now we have a type we can do something with:</p> <pre><code>omni_types=# create table geoms as values \n('point(10,10)'::geom),('circle(&lt;10,10,10&gt;)'::geom),('circle(&lt;20,10,10&gt;)'::geom);\nSELECT 3\n</code></pre> <p>Let's select all shapes that have an X coordinate set to 10:</p> <pre><code>omni_types=# select * from geoms where\n                  (omni_types.variant(column1) = 'point'::regtype \n                   and (column1::point)[0] = 10) \n               or (omni_types.variant(column1) = 'circle'::regtype \n                   and (point(column1::circle))[0] = 10);\n       column1\n----------------------\n point((10,10))\n circle(&lt;(10,10),10&gt;)\n(2 rows)\n</code></pre> <p>There are still a few issues to work out (adding binary send/recv support, ensuring proper TOAST support, convenience API, etc.) but I think it's a worthy prototype to play with. </p>"},{"location":"blog/2023/04/09/sum-types-in-postgres/#try-it-out","title":"Try it out","text":"<p>You can try it out as part of latest Omnigres builds:</p> <pre><code>docker volume create omnigres\ndocker run --name omnigres -e POSTGRES_PASSWORD=omnigres -e POSTGRES_USER=omnigres \\\n                           -e POSTGRES_DB=omnigres --mount source=omnigres,target=/var/lib/postgresql/data \\\n           -p 5432:5432 -p 8080:8080 --rm ghcr.io/omnigres/omnigres-slim:latest\n# Now you can connect to it:\npsql -h localhost -p 5432 -U omnigres omnigres # password is `omnigres`\n</code></pre> <p>In <code>psql</code>:</p> <pre><code>omnigres=# create extension omni_types;\n</code></pre> <p>You can read more about using the sum types in the documentation.</p> <ol> <li> <p>PostGIS has its own <code>geometry</code> type but I am using this is an example.\u00a0\u21a9</p> </li> <li> <p><code>omni_types</code> extension in Omnigres\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2017/09/04/solving-problems-one-commit-at-a-time/","title":"Solving Problems, One Commit at a Time","text":"<p>If you have been following some of my projects in the past year or so, you might have noticed that the my commit messages changed considerably and they no longer look typical. Take a look at this one:</p> <p></p> Screenshot transcription <pre><code>Problem: use of return stack to accumulate values reverses them\nAs of #340 we now must use &gt;R and R&gt; to pass the data out of a DOWHILE-like\nloop. It's a working solution, however, it also means that if one to process\nthe return stack later on (in another loop), the values will be picked in a LIFO order.\n\nNot a huge problem by itself, but in PumpkinDB, the loops are often used to\niterate over sorted keys and this means that the results of such \"scans\" will\nbe in a reverse order.\n\nSolution: replace return stack with a (double-ended) queue.\n\nThis replaces `&gt;R` and `R&gt;` to `&gt;Q` and `Q&gt;` (\"to the back of the queue\"\nand \"from the back of the queue\", respectively) and adds `&lt;Q` and `Q&lt;`\n(\"to the front of the queue\" and \"from the front of the queue\", respectively)\n\nWith this change it is possible to collect and process results in the same\norder.\n\nCloses #341\nCloses #342\n</code></pre> <p>I shamelessly copied this commit message format from Pieter Hintjens\u2019 C4 unprotocol. At first, this kind of commit message felt weird. Why would we want to begin every commit with a word \u201cProblem\u201d? What\u2019s in it for me?</p> <p>Turns out, there are multiple benefits to sticking to this ceremony.</p> <p>Firstly, it gives you, the author of the change, an opportunity to understand what you are actually working on. Often times, if I am not quite sure what am I trying to accomplish, I\u2019d prepare a WIP commit or a GitHub issue that simply and unambigously states the problem.</p> <p>As opposed to purely self-documenting commit messages (say, \u201cAdded a reference to a user ID\u201d) we\u2019re documenting the reason why we bothered to torture the keyboard in the first place.</p> <p>As a consequence, this kind of scoping prevents us from having to figure out the level of granularity of commit messages. A friend of mine recently complained to me that his new hire is pushing tons of very small commits and that was bothering him. As annoying as that may be, this case illuminates the underlying problem: where do we draw the line in the sand if our changes are, say, touching different parts of the system?</p> <p>With this approach, we can draw that line at solving one and only one problem at a time.</p> <p>Secondly, it gives you a context to every past change. Lets be frank, how easy is it for you to remember why you, let alone another person, did this or that change, three months later? A year? Ten?</p> <p>By recording the entire context, we are giving everybody who\u2019ll be working with the codebase (contributors, users, etc.) a way to figure out why changes happened in the first place. In some cases it would relieve frustration, in other, it\u2019ll help figuring out which problems aren\u2019t problems anymore and therefore, what kind of improvements can be made.</p> <p>Thirdly, it\u2019s a great way to understand incoming changes. If you maintained a collaboratively developed project before, how many times have you received a pull request that left you wondering \u201cwhy on Earth?..\u201d</p> <p>By focusing on (and explicitly stating) the problem this change is solving, you can help the maintainers understand your perspective and (surprise, surprise!) make it easier for your change to be successfully merged.</p> <p>Which leads me to another point. Not being able to understand each other is often the reason why people can be less than excellent to each other. This being a big issue in the community these days, working on contributing to a pool of shared meaning will be a net benefit.</p> <p>I\u2019ve heard some concerns about using this format, mostly boiling down to \u201cmy team mates aren\u2019t producing insanely great commit messages right now, how can I expect them to adhere to a much more demanding format?\u201d</p> <p>On the surface, this makes sense. However, the beauty is in rigidity of this format. It gives you simple rules, a framework for making statements of a very specific kind. Once most of common mistakes are learned, this becomes a second nature. As far as I can tell, the most common issue is writing messages like \u201cProblem: no author_id\u201d / \u201cSolution: add author_id\u201d. Obviously, this is a problem expressed through the solution, so an extra step has to be made to understand why there is this change.</p> <p>Try it out on an open source or a side-project!</p> <p>Bonus tip: when starting a new project, make an empty first commit with the Problem/Solution statement in its message. This will help others understand WHY this project has been started and WHAT does it do.</p>"},{"location":"blog/2014/04/21/team-uncommunication/","title":"Team Uncommunication","text":"<p>The art of silence</p> <p>Last year, my friends and I spent a few days by a lake on Vancouver Island. The house we were renting came with two canoes and, naturally, one of my friends and I ended up paddling one. Being quite new to canoes, we were at first struggling to keep the thing straight and move swiftly.</p> <p></p> Canoe, picture by oldtowncanoe.com <p>Why? Because we tried to communicate it wrong. We started off by giving each other infrequent commands and doing what felt right. Net result? We were almost spinning around. Then we decided to go for the \u201cleadership model\u201d where the one on the stern seat (\"stroke\") orchestrates. That was a little better, but it was still frustrating as hell.</p> <p>What helped us and ultimately brought joy to the experience was a simple idea of a steering agreement. As opposed to a kayak paddle, a canoe paddle only allows you to paddle on one side at a time. We decided to always paddle on opposites sides (duh!) and whenever either of us was overpaddling, the other person was having a little break. After a few minutes, we were going in a straight line, having a lot of fun and we didn\u2019t need to communicate our paddling at all!</p> <p>Why do I still remember this little story? One of the reasons is because it is so illustrative to the whole \"team communication\" debate. Over years, I\u2019ve seen different teams communicate in different ways. The root of the problem seems to be in the wrong choice of word (\"communication\"). Many of us have simply assumed that this is about talking.</p> <p>But is that a right way to think about it? There is a lot of artifacts brought to us by this concept. Morning standup meetings with recital of what can be easily extracted from a well organized scrum board. Relying on in-person communications and, as a result, building a no remote people allowed culture that often leads to poor documenting efforts. Leadership struggles. Picking the right tool for chatting.</p> <p>What if instead of relying on constant talking, we focus on defining easy to follow game rules and using technology to facilitate this? Imagine the joy of being able to stay uninterrupted when you need that most. Imagine the ability to hire outside of a 50 miles radius. Imagine being able to let new hires onboard themselves. Imagine shipping ahead of the schedule (ok, I might have made that one up :).</p> <p>Establishing this will not be easy and it will require some ongoing maintenance, but the net benefit will be seriously outweighing the expenditures. For the moment, I am deliberately not going to make any practical suggestions on what to do to achieve the zen of team uncommunication. I want this concept to continue roaming free of constraints for a little while so that we can be as creative and critical as possible. I choose to stay inquisitive about enabling teamwork the right way. I choose to stay a newbie.</p>"},{"location":"blog/2023/01/07/why-not-rust-for-omnigres/","title":"Why not Rust for Omnigres?","text":"<p>Omnigres is a new project to turn Postgres into a complete development and production deployment platform. I've started it to reflect on the complexity and inefficiencies plaguing modern business software development.</p> <p>As an aging (and sometimes cranky!) developer, I crave simplicity. But that's a topic for another post. Here I wanted to address a common question:</p> <p>Why didn't you implement this in Rust?</p> <p>It's a great question, considering I've been using Rust for a number of years now, and I generally advocate its use for its rich ecosystem, safety and tooling. I actively contribute to pgx, a library for building Postgres extensions in Rust. Yet, Omnigres appears to be all done in C.</p> <p>Here's why.</p> <p>Beware: the list has a lot of subjective and controversial opinions.</p>"},{"location":"blog/2023/01/07/why-not-rust-for-omnigres/#making-a-good-wrapper-pgx-is-a-lot-of-work","title":"Making a good wrapper (pgx) is a lot of work","text":"<p>While it's getting there, when you try to build something pushing the boundaries, pgx may not yet provide sufficient API coverage, so you have to resort to unsafe FFI. It drove me to make a number of contributions to it. Still, since it takes a while to refine them enough to be considered well-rounded, it impedes general development velocity.</p> <p>Making a good, safe wrapper for such a complex project as Postgres is a significant undertaking. It'll take however long it takes. That's why I am wholeheartedly supporting it and spending time evolving it.</p> <p>But as of today, I want to be able to move fast.</p>"},{"location":"blog/2023/01/07/why-not-rust-for-omnigres/#postgres-is-quirky-c","title":"Postgres is [quirky] C","text":"<p>Its whole internal API is designed to be consumed by C code, and Postgres doesn't know anything about your other language.</p> <p>It uses <code>setjmp/longjmp</code> for exceptions (so you'd have to roll your own guards to unwind the stack). It has its own memory management system. It manages the SPI stack. It has lots of mutable global variables!</p> <p>So, to consume it from C is only natural, even though it can be dangerous.</p>"},{"location":"blog/2023/01/07/why-not-rust-for-omnigres/#dependencies","title":"Dependencies","text":"<p>Between not really knowing what's inside of your dependencies and obnoxious compile times, there's a reason for having less dependencies.</p> <p>Rust's Cargo makes it terribly easy to add dependencies, so there's no tax on adding yet another library. This is generally seen as a good thing in the industry, but I keep coming back to the idea that less is better.</p> <p>If, however, you had to pay for every dependency, we'd use a lot less of them, only when necessary. C is excellent in this department! I gave up on a number of dependencies simply because I had difficulties integrating them into my build system. So I see this is a filter for the need and the quality of the dependency.</p>"},{"location":"blog/2023/01/07/why-not-rust-for-omnigres/#fast-compile-times","title":"Fast compile times","text":"<p>Rust compiler is notoriously slow. Pgx brings its own complexity that makes rapid iteration nearly impossible. My builds are extremely fast with C. It's a simple language, and I tend to add very few dependencies.</p>"},{"location":"blog/2023/01/07/why-not-rust-for-omnigres/#complex-language-fatigue","title":"Complex language fatigue","text":"<p>I've been developing software for a few decades now and am growing tired of complex languages. Going back to C feels like taking a vacation and stopping chasing ideals and just focusing on the problem at hand.</p> <p>If Postgres (or Linux kernel) can be done in C, I am sure a few extensions could, too!</p>"},{"location":"blog/2023/01/07/why-not-rust-for-omnigres/#formal-verification-methods","title":"Formal verification methods","text":"<p>There are projects that are focusing on bringing this to Rust, but there are some great static analysis and formal verification tools that have been used in C for quite a while. I fancy Frama-C and try to add it bit by bit to Omnigres where I think it may be necessary.</p>"},{"location":"blog/2023/01/07/why-not-rust-for-omnigres/#rust-safeties-are-slightly-less-relevant-in-postgres","title":"Rust safeties are slightly less relevant in Postgres","text":"<p>Postgres is inherently single-threaded and therefore, \"fearless concurrency\" is not buying us much. Sure, you can build an extension that'll communicate with other processes and threads over shared memory, but this is not much of a concern at the core database level.</p> <p>Postgres has its own memory management story, which is quite good. Instead of having a single allocation pool, it has memory contexts that are established temporarily or for a scope. When they are deleted, all the memory allocated in them is released. So memory leaks are not necessarily a concern. Rust doesn't necessarily consider leaks a safety issue, but it's nice to know we will not crash the server with an OOM.</p> <p>Of course, there's still use-after-free, which you have to be careful about. This is where tools like AddressSanitizer can come in very handy.</p> <p>I've seen that most of my crashes in the extensions were coming from misusing some Postgres API, and I was getting a lot of them in Rust as well.</p> <p>But Yurii, isn't C a dangerous and flawed language?</p> <p>Yes, it is both dangerous and flawed. In general, I wouldn't build new projects in C, but after months of going back on forth on this, I think, ultimately, building a project on top of Postgres in C is reasonable.</p> <p>Did you consider Zig?</p> <p>I absolutely did. I am very interested in Zig (and have previously developed some small projects in it) and I keep track of its development. But I ultimately didn't want to fight both Postgres and Zig at the same time.</p> <p>Did you consider Nim?</p> <p>You bet! The attraction was that it compiles to C, and I can emit C code where necessary, obliterating the need to replicate certain tricks. But it's a complex, big language, and I am not confident about its own bugs, obscurities and upgrade stories (Nim 2 is coming!)</p> <p>Knowing how Rust crowd behaves sometimes, there may be those inclined to persuade me to change my opinion or prove me wrong. Of course, they are free to do so, but I'd prefer to focus on getting things done!</p> <p>Ultimately, I think Rust is great. Omnigres will have first-class support for it and will promote its use and the use of pgx for building extensions. Update: PL/Rust is now being shipped with Omnigres!</p>"},{"location":"opensource/","title":"Open Source Involvement","text":"<p>The order of entries on this page is an attempt to highlight projects based on recency and impact.</p> <p>If any of these sound exciting to you and you'd like to support my work, please consider sponsoring my work <sup>1</sup>.</p>"},{"location":"opensource/#current","title":"Current","text":""},{"location":"opensource/#omnigres","title":"Omnigres","text":"<p>Omnigres makes Postgres a complete application platform. You can deploy a single database instance and it can host your entire application, scaling as needed.</p> <p>Note</p> <p>I am most actively involved in this project at the moment.</p>"},{"location":"opensource/#libfyaml","title":"libfyaml","text":"<p>I am actively contributing to this YAML library for C.</p>"},{"location":"opensource/#pgrx","title":"pgrx","text":"<p>Previously known as <code>pgx</code></p> <p>I contribute to <code>pgrx</code>, a library to build Postgres extensions in Rust with, on an ongoing basis, primarily driven by needs of clients and my interests.</p>"},{"location":"opensource/#pg_graphql","title":"pg_graphql","text":"<p>I am contributing to <code>pg_graphql</code> as requested. More contributions expected.</p>"},{"location":"opensource/#wrappers","title":"wrappers","text":"<p>I've helped refactoring Supabase wrappers (a framework and a collection of Foreign Data Wrappers for Postgres)</p>"},{"location":"opensource/#postgres","title":"Postgres","text":"<p>So far, I've proposed two small patches:</p> <ul> <li>Extend the length of BackgroundWorker.bgw_library_name</li> <li>Allow Postgres to pick an unused port to listen</li> </ul>"},{"location":"opensource/#git-paravendor","title":"git-paravendor","text":"<p>I am developing a tool to vendorize dependencies alongside in the project repository.</p> <p>This helps with workflows in absence of a good internet connection, and ensures that dependencies that are gone are not going to have an immediate disrupting impact on your project.</p>"},{"location":"opensource/#ideas","title":"Ideas","text":"<p>These are some of the projects I would like to work on, but haven't started yet. If any of these sound exciting to you and you'd like them to be implemented, please consider sponsoring my work <sup>1</sup>.</p>"},{"location":"opensource/#postgres-related","title":"Postgres-related","text":""},{"location":"opensource/#in-memory-access-method","title":"In-memory access method","text":"<p>For cases like long-term in-memory caches, it'd be great to not have them stored on disk at all (that rules out unlogged tables) and have them persisted across sessions (that rules out temporary tables).</p> <p>I am eyeing the idea of implementing a table/index access method that will work in shared memory.</p> Notes <p>I think that in order to make it easier to deploy, it should not use Postgres shared memory and instead work with operating system's shared memory directly. </p>"},{"location":"opensource/#postgres-patches","title":"Postgres Patches","text":""},{"location":"opensource/#object-synomyms","title":"Object Synomyms","text":"<p>Sometimes naming objects the way they were designed (either in Postgres or in an extension) is problematic, especially when you need to qualify name to the schema or import the whole schema by adding it to search path.</p> <p>Oracle has support for synonyms for this exact reason.</p> <p>Postgres doesn't, but it'd be great to have it.</p>"},{"location":"opensource/#enable-overriding-hard-coded-paths-in-postgresqlconf","title":"Enable overriding hard-coded paths in <code>postgresql.conf</code>","text":"<p>The fact that many paths are hard-coded in postgres during compile-time can be frustrating at times. No way to change that without recompiling it. It'd be great if <code>postgres</code> and <code>pg_config</code> were able to do this. </p> How can this be implemented? <p>There's <code>src/port/pg_config_paths.h</code> generated when configuring Postgres that hard-codes all these paths.</p> <p>What I've done so far is augmenting it with something like:</p> <pre><code>const char * _PGSHAREDIR = PGSHAREDIR;\n#undef PGSHAREDIR\n#define PGSHAREDIR ((const char *)getenv(\"PGSHAREDIR\") ? : _PGSHAREDIR)\n</code></pre> <p>Now, of course, this <code>getenv</code> call would have to be changed to attempt retrieving the string from the configuration setting.</p> <p>Also, <code>pg_config</code> would need to be able to take an option to point to <code>postgresql.conf</code>.</p>"},{"location":"opensource/#shell-scripting-language-compiler","title":"Shell Scripting Language Compiler","text":"<p>A simple imperative language intended to be used for shell scripting. However, it won't have an interpret. Instead, it'll compile code to targets like Bash, Dockerfile, etc. The idea is to have a simple and sane language for shell scripting without requiring users to install an interpreter.</p> <p>Features may include some basic support for parallelization, JSON <sup>2</sup> and tabular data processing, etc.</p>"},{"location":"opensource/#better-c-as-a-c-superset","title":"Better C as a C superset","text":"<p>Codename \"Spicy Language\"</p> <p>Some people have tried to solve C's problems by creating new languages (Zig, D), some went even a bit further than just C (Rust, Nim) but I wonder if C itself can be saved using some magic. I really don't fancy the idea of keeping rewriting libraries in the new languages every couple of years.</p> <p>What I want is Better C that is still 100% C. That is, it compiles existing C code just fine. It doesn't change any of the existing semantics.  But it does offer extra features on top of it. </p> <p>First thing I would like to fix is with regard to the flat namespaces and conflicts. Perhaps something along the lines of namespaces (especially for dependencies), external symbol renaming, etc. These things make our lives just miserable at the worst time.</p> <p>I'd like it to fix scope exits (without out-of-order statements), and a few other things.</p> <p>It can also perhaps with minor annoyances of C such as the necessity for forward declarations.</p> <p>It'd be great to implement Spicy as a set of syntax transformers that can operate both on the source code level but also preserve state across translation units to fix issues that arise outside of the translation unit boundaries. It can be developed as a drop-in replacement for <code>cc</code>, <code>ld</code> and other tools to be aware of some of the user's intent <sup>3</sup>. </p>"},{"location":"opensource/#virtual-machine-shim-for-libcrun","title":"Virtual Machine Shim for <code>libcrun</code>","text":"<p>Using containers on a development machine is fun if it is a Linux machine. Otherwise, one has to resort to virtual machines. </p> <p>Lately, I've grown to appreciate having less dependencies that require separate installation. For that reason, relying on docker, podman, runc, crun or other tools as binaries doesn't seem attractive anymore. I want to be able to use <code>libcrun</code> on both Linux and macOS alike. With no change in API. Perhaps even use Virtualization Framework on macOS <sup>4</sup>.</p>"},{"location":"opensource/#application-operating-system","title":"Application Operating System","text":"<p>Codename \"ParaOS\"</p> <p>We build software systems on top of many layers (in the name of focus, efficiency and to avoid reinventing the wheel.) Battlefield-tested foundation that addresses common needs is a great time, money and energy saver.</p> <p>However, many of the layers we use today have been designed for a different environment, needs and deployment size.</p> <p>We (predominantly) write code in programming languages that were intended to make programs that run on a single computer, put that code into files and deploy instances of operating systems, the foundations of which were designed about 50 years ago, primarily concerning itself with serving the resources (CPU time, memory, persistence, networking and other peripherals) of an underlying computer to multiple users and their programs.</p> <p>The systems we develop these days span multiple nodes and heavily rely on lage-scale, highly-available persistence capabilities as opposed to dealing with local files, multiuser access, etc.</p> <p>So, what if built an operating system for these applications?</p> <p>Unlike traditional operating systems, this system would focus on providing building blocks (operational guarantees, APIs, code mobility, etc.) for building applications that span multiple computers as opposed to focusing on serving the underlying hardware to applications as in programs in user space.</p>"},{"location":"opensource/#past","title":"Past","text":"<p>Below are the contributions that I am not using or involved in actively anymore (but others could be). Some projects maybe in a derelict shape.</p>"},{"location":"opensource/#ruby-on-rails","title":"Ruby on Rails","text":"<p>A few minor contributions in the past and recently.</p>"},{"location":"opensource/#hotwire","title":"Hotwire","text":"<p>I've contributed to Turbo Rails a bit when developing my startup HackerIntro.</p>"},{"location":"opensource/#wasmrs","title":"wasm.rs","text":"<p>I've tried to build a few WebAssembly-related libraries and a community. The attempt has fizzled out but I still think there's something to it. At least the domain name is interesting!</p>"},{"location":"opensource/#graphql-java-annotations","title":"graphql-java-annotations","text":"<p>I've conceived the project long time ago and it has since been taken over.</p>"},{"location":"opensource/#elixir","title":"Elixir","text":"<p>In the early days, I've contributed to Elixir programming language as I was deeply involved in the Erlang community and was looking for ways to improve my code.</p>"},{"location":"opensource/#erlzmq2","title":"erlzmq2","text":"<p>I've built this better-performing ZeroMQ wrapper many years ago.</p> <ol> <li> <p>You can create a custom monthly or one-time amount. Send me a message to discuss details.\u00a0\u21a9\u21a9</p> </li> <li> <p>Parsing JSON in pure Bash sounds like fun, right? People have done it, though.   Maybe it can be written in the language itself.\u00a0\u21a9</p> </li> <li> <p>There are some fun ideas like delaying compiling object files until they are used for something so that   they can be manipulated before the final steps.\u00a0\u21a9</p> </li> <li> <p>I know it doesn't have a great API and it has tons of deficiencies. But I think I may have a workaround or two.\u00a0\u21a9</p> </li> </ol>"},{"location":"opensource/ideas/","title":"Open Source Ideas","text":"<p>These are some of the projects I would like to work on, but haven't started yet. If any of these sound exciting to you and you'd like them to be implemented, please consider sponsoring my work <sup>1</sup>.</p>"},{"location":"opensource/ideas/#postgres-related","title":"Postgres-related","text":""},{"location":"opensource/ideas/#in-memory-access-method","title":"In-memory access method","text":"<p>For cases like long-term in-memory caches, it'd be great to not have them stored on disk at all (that rules out unlogged tables) and have them persisted across sessions (that rules out temporary tables).</p> <p>I am eyeing the idea of implementing a table/index access method that will work in shared memory.</p> Notes <p>I think that in order to make it easier to deploy, it should not use Postgres shared memory and instead work with operating system's shared memory directly. </p>"},{"location":"opensource/ideas/#postgres-patches","title":"Postgres Patches","text":""},{"location":"opensource/ideas/#object-synomyms","title":"Object Synomyms","text":"<p>Sometimes naming objects the way they were designed (either in Postgres or in an extension) is problematic, especially when you need to qualify name to the schema or import the whole schema by adding it to search path.</p> <p>Oracle has support for synonyms for this exact reason.</p> <p>Postgres doesn't, but it'd be great to have it.</p>"},{"location":"opensource/ideas/#enable-overriding-hard-coded-paths-in-postgresqlconf","title":"Enable overriding hard-coded paths in <code>postgresql.conf</code>","text":"<p>The fact that many paths are hard-coded in postgres during compile-time can be frustrating at times. No way to change that without recompiling it. It'd be great if <code>postgres</code> and <code>pg_config</code> were able to do this. </p> How can this be implemented? <p>There's <code>src/port/pg_config_paths.h</code> generated when configuring Postgres that hard-codes all these paths.</p> <p>What I've done so far is augmenting it with something like:</p> <pre><code>const char * _PGSHAREDIR = PGSHAREDIR;\n#undef PGSHAREDIR\n#define PGSHAREDIR ((const char *)getenv(\"PGSHAREDIR\") ? : _PGSHAREDIR)\n</code></pre> <p>Now, of course, this <code>getenv</code> call would have to be changed to attempt retrieving the string from the configuration setting.</p> <p>Also, <code>pg_config</code> would need to be able to take an option to point to <code>postgresql.conf</code>.</p>"},{"location":"opensource/ideas/#shell-scripting-language-compiler","title":"Shell Scripting Language Compiler","text":"<p>A simple imperative language intended to be used for shell scripting. However, it won't have an interpret. Instead, it'll compile code to targets like Bash, Dockerfile, etc. The idea is to have a simple and sane language for shell scripting without requiring users to install an interpreter.</p> <p>Features may include some basic support for parallelization, JSON <sup>2</sup> and tabular data processing, etc.</p>"},{"location":"opensource/ideas/#better-c-as-a-c-superset","title":"Better C as a C superset","text":"<p>Codename \"Spicy Language\"</p> <p>Some people have tried to solve C's problems by creating new languages (Zig, D), some went even a bit further than just C (Rust, Nim) but I wonder if C itself can be saved using some magic. I really don't fancy the idea of keeping rewriting libraries in the new languages every couple of years.</p> <p>What I want is Better C that is still 100% C. That is, it compiles existing C code just fine. It doesn't change any of the existing semantics.  But it does offer extra features on top of it. </p> <p>First thing I would like to fix is with regard to the flat namespaces and conflicts. Perhaps something along the lines of namespaces (especially for dependencies), external symbol renaming, etc. These things make our lives just miserable at the worst time.</p> <p>I'd like it to fix scope exits (without out-of-order statements), and a few other things.</p> <p>It can also perhaps with minor annoyances of C such as the necessity for forward declarations.</p> <p>It'd be great to implement Spicy as a set of syntax transformers that can operate both on the source code level but also preserve state across translation units to fix issues that arise outside of the translation unit boundaries. It can be developed as a drop-in replacement for <code>cc</code>, <code>ld</code> and other tools to be aware of some of the user's intent <sup>3</sup>. </p>"},{"location":"opensource/ideas/#virtual-machine-shim-for-libcrun","title":"Virtual Machine Shim for <code>libcrun</code>","text":"<p>Using containers on a development machine is fun if it is a Linux machine. Otherwise, one has to resort to virtual machines. </p> <p>Lately, I've grown to appreciate having less dependencies that require separate installation. For that reason, relying on docker, podman, runc, crun or other tools as binaries doesn't seem attractive anymore. I want to be able to use <code>libcrun</code> on both Linux and macOS alike. With no change in API. Perhaps even use Virtualization Framework on macOS <sup>4</sup>.</p>"},{"location":"opensource/ideas/#application-operating-system","title":"Application Operating System","text":"<p>Codename \"ParaOS\"</p> <p>We build software systems on top of many layers (in the name of focus, efficiency and to avoid reinventing the wheel.) Battlefield-tested foundation that addresses common needs is a great time, money and energy saver.</p> <p>However, many of the layers we use today have been designed for a different environment, needs and deployment size.</p> <p>We (predominantly) write code in programming languages that were intended to make programs that run on a single computer, put that code into files and deploy instances of operating systems, the foundations of which were designed about 50 years ago, primarily concerning itself with serving the resources (CPU time, memory, persistence, networking and other peripherals) of an underlying computer to multiple users and their programs.</p> <p>The systems we develop these days span multiple nodes and heavily rely on lage-scale, highly-available persistence capabilities as opposed to dealing with local files, multiuser access, etc.</p> <p>So, what if built an operating system for these applications?</p> <p>Unlike traditional operating systems, this system would focus on providing building blocks (operational guarantees, APIs, code mobility, etc.) for building applications that span multiple computers as opposed to focusing on serving the underlying hardware to applications as in programs in user space.</p> <ol> <li> <p>You can create a custom monthly or one-time amount. Send me a message to discuss details.\u00a0\u21a9</p> </li> <li> <p>Parsing JSON in pure Bash sounds like fun, right? People have done it, though.   Maybe it can be written in the language itself.\u00a0\u21a9</p> </li> <li> <p>There are some fun ideas like delaying compiling object files until they are used for something so that   they can be manipulated before the final steps.\u00a0\u21a9</p> </li> <li> <p>I know it doesn't have a great API and it has tons of deficiencies. But I think I may have a workaround or two.\u00a0\u21a9</p> </li> </ol>"},{"location":"opensource/ideas_/","title":"Ideas","text":"<p>These are some of the projects I would like to work on, but haven't started yet. If any of these sound exciting to you and you'd like them to be implemented, please consider sponsoring my work <sup>1</sup>.</p>"},{"location":"opensource/ideas_/#postgres-related","title":"Postgres-related","text":""},{"location":"opensource/ideas_/#in-memory-access-method","title":"In-memory access method","text":"<p>For cases like long-term in-memory caches, it'd be great to not have them stored on disk at all (that rules out unlogged tables) and have them persisted across sessions (that rules out temporary tables).</p> <p>I am eyeing the idea of implementing a table/index access method that will work in shared memory.</p> Notes <p>I think that in order to make it easier to deploy, it should not use Postgres shared memory and instead work with operating system's shared memory directly. </p>"},{"location":"opensource/ideas_/#postgres-patches","title":"Postgres Patches","text":""},{"location":"opensource/ideas_/#object-synomyms","title":"Object Synomyms","text":"<p>Sometimes naming objects the way they were designed (either in Postgres or in an extension) is problematic, especially when you need to qualify name to the schema or import the whole schema by adding it to search path.</p> <p>Oracle has support for synonyms for this exact reason.</p> <p>Postgres doesn't, but it'd be great to have it.</p>"},{"location":"opensource/ideas_/#enable-overriding-hard-coded-paths-in-postgresqlconf","title":"Enable overriding hard-coded paths in <code>postgresql.conf</code>","text":"<p>The fact that many paths are hard-coded in postgres during compile-time can be frustrating at times. No way to change that without recompiling it. It'd be great if <code>postgres</code> and <code>pg_config</code> were able to do this. </p> How can this be implemented? <p>There's <code>src/port/pg_config_paths.h</code> generated when configuring Postgres that hard-codes all these paths.</p> <p>What I've done so far is augmenting it with something like:</p> <pre><code>const char * _PGSHAREDIR = PGSHAREDIR;\n#undef PGSHAREDIR\n#define PGSHAREDIR ((const char *)getenv(\"PGSHAREDIR\") ? : _PGSHAREDIR)\n</code></pre> <p>Now, of course, this <code>getenv</code> call would have to be changed to attempt retrieving the string from the configuration setting.</p> <p>Also, <code>pg_config</code> would need to be able to take an option to point to <code>postgresql.conf</code>.</p>"},{"location":"opensource/ideas_/#shell-scripting-language-compiler","title":"Shell Scripting Language Compiler","text":"<p>A simple imperative language intended to be used for shell scripting. However, it won't have an interpret. Instead, it'll compile code to targets like Bash, Dockerfile, etc. The idea is to have a simple and sane language for shell scripting without requiring users to install an interpreter.</p> <p>Features may include some basic support for parallelization, JSON <sup>2</sup> and tabular data processing, etc.</p>"},{"location":"opensource/ideas_/#better-c-as-a-c-superset","title":"Better C as a C superset","text":"<p>Codename \"Spicy Language\"</p> <p>Some people have tried to solve C's problems by creating new languages (Zig, D), some went even a bit further than just C (Rust, Nim) but I wonder if C itself can be saved using some magic. I really don't fancy the idea of keeping rewriting libraries in the new languages every couple of years.</p> <p>What I want is Better C that is still 100% C. That is, it compiles existing C code just fine. It doesn't change any of the existing semantics.  But it does offer extra features on top of it. </p> <p>First thing I would like to fix is with regard to the flat namespaces and conflicts. Perhaps something along the lines of namespaces (especially for dependencies), external symbol renaming, etc. These things make our lives just miserable at the worst time.</p> <p>I'd like it to fix scope exits (without out-of-order statements), and a few other things.</p> <p>It can also perhaps with minor annoyances of C such as the necessity for forward declarations.</p> <p>It'd be great to implement Spicy as a set of syntax transformers that can operate both on the source code level but also preserve state across translation units to fix issues that arise outside of the translation unit boundaries. It can be developed as a drop-in replacement for <code>cc</code>, <code>ld</code> and other tools to be aware of some of the user's intent <sup>3</sup>. </p>"},{"location":"opensource/ideas_/#virtual-machine-shim-for-libcrun","title":"Virtual Machine Shim for <code>libcrun</code>","text":"<p>Using containers on a development machine is fun if it is a Linux machine. Otherwise, one has to resort to virtual machines. </p> <p>Lately, I've grown to appreciate having less dependencies that require separate installation. For that reason, relying on docker, podman, runc, crun or other tools as binaries doesn't seem attractive anymore. I want to be able to use <code>libcrun</code> on both Linux and macOS alike. With no change in API. Perhaps even use Virtualization Framework on macOS <sup>4</sup>.</p>"},{"location":"opensource/ideas_/#application-operating-system","title":"Application Operating System","text":"<p>Codename \"ParaOS\"</p> <p>We build software systems on top of many layers (in the name of focus, efficiency and to avoid reinventing the wheel.) Battlefield-tested foundation that addresses common needs is a great time, money and energy saver.</p> <p>However, many of the layers we use today have been designed for a different environment, needs and deployment size.</p> <p>We (predominantly) write code in programming languages that were intended to make programs that run on a single computer, put that code into files and deploy instances of operating systems, the foundations of which were designed about 50 years ago, primarily concerning itself with serving the resources (CPU time, memory, persistence, networking and other peripherals) of an underlying computer to multiple users and their programs.</p> <p>The systems we develop these days span multiple nodes and heavily rely on lage-scale, highly-available persistence capabilities as opposed to dealing with local files, multiuser access, etc.</p> <p>So, what if built an operating system for these applications?</p> <p>Unlike traditional operating systems, this system would focus on providing building blocks (operational guarantees, APIs, code mobility, etc.) for building applications that span multiple computers as opposed to focusing on serving the underlying hardware to applications as in programs in user space.</p> <ol> <li> <p>You can create a custom monthly or one-time amount. Send me a message to discuss details.\u00a0\u21a9</p> </li> <li> <p>Parsing JSON in pure Bash sounds like fun, right? People have done it, though.   Maybe it can be written in the language itself.\u00a0\u21a9</p> </li> <li> <p>There are some fun ideas like delaying compiling object files until they are used for something so that   they can be manipulated before the final steps.\u00a0\u21a9</p> </li> <li> <p>I know it doesn't have a great API and it has tons of deficiencies. But I think I may have a workaround or two.\u00a0\u21a9</p> </li> </ol>"},{"location":"postgres/","title":"Postgres","text":"<p>I am currently involved with a number of things centered around the idea of Postgres as a technology platform. With its steady development and the growing extension ecosystem, it is well primed for taking on new kinds of functionality.</p> <p>I blog about Postgres-related matters.</p>"},{"location":"postgres/#my-current-patches","title":"My current patches","text":"<ul> <li>Extend the length of BackgroundWorker.bgw_library_name</li> <li>Allow Postgres to pick an unused port to listen</li> </ul>"},{"location":"postgres/patches/","title":"Patches","text":"<ul> <li>Extend the length of BackgroundWorker.bgw_library_name</li> <li>Allow Postgres to pick an unused port to listen</li> </ul>"},{"location":"startups/","title":"Startups","text":"<p>My entrepreneurship journey is closely tied with the tech startup world. I have done a few ventures in the past, to varying degress of success.</p> <p>Why do I do this?</p> <p>For me, it is about the unlimited range of challenges and opportunities every day brings. Wearing many hats, and being ultimately responsible how well you will perform mentally, physically and financial is up to you. It's very raw, in a sense. You get what you put in, multiplied by the luck factor. Generally, no safety net. You're responsible for what happens.</p>"},{"location":"startups/current/","title":"Current Startups","text":""},{"location":"startups/current/#omnigres-2022-","title":"Omnigres (2022-)","text":"<p>Omnigres makes Postgres a complete application platform. You can deploy a single database instance and it can host your entire application, scaling as needed.</p> <p>I started Omnigres because data-centric application development was getting more complex and I was seeking an antidote for that. I believe Postgres can be a great platform for such applications, especially considering its popularity. It needs a much improved DX and a constellations of extensions that would make it just perfect. That's Omnigres.</p>"},{"location":"startups/ideas/","title":"Startup Ideas","text":""},{"location":"startups/ideas/#hands-free-writing","title":"Hands-free writing","text":"<p>As somebody who doesn't have a lot of spare time I feel like I could have used a bit of the \"idle\" time for something useful, like writing articles or blog posts. However, this \"idle\" time usually means I am not home, not having my computer with me. At best, I have my phone.</p> <p>But writing on the phone sucks. Speech recognition isn't great either. It works to a degree, but requires painful editing. </p> <p>The idea is to give a multi-model, hands-free-first writing experience on the phone and other devices. Try to employ GPT to fix speech recognition mistakes, pad coarse input, perform text manipulation.</p>"},{"location":"startups/ideas/#save-on-kids","title":"Save on Kids","text":"<p>Have saveonkids.com &amp; saveonkids.ca</p> <p>This is similar to the existing brick-and-mortar model where parents can buy and sell used children's clothes, shoes and toys. However, instead of having to shop in person, turn this into an e-commerce store. Both local and shipping models are worth exploring.</p>"},{"location":"startups/past/","title":"Past Startups","text":""},{"location":"startups/past/#hackerintro","title":"HackerIntro (2021-2023)","text":"<p>Started as a way to deal with inefficiencies and annoyances of finding developers and finding jobs for those said developers.</p> <p>Its current main product is Resume Search, it crawls and indexes developer resumes 24/7 and provides access to this database to subscribers.</p> <p>I wasn't able to grow it sufficiently and have shut it down.</p>"},{"location":"startups/past/#trustatom","title":"Trustatom (2014-2015)","text":"<p>After deparing Trustatom I travelled to Asia to reset. While in Japan, I came up with the idea of a small service that would do a limited set of smart contracts based on Bitcoin. </p> Main pageJoint EscrowsIP Cerificate <p></p> <p></p> <p></p> <p>With limited success selling this, I've raised some money and pivoted to a more specific niche of fundraising and created a small product called Streamline.vc. The product was used to routinely register information (such as ownership structure, KPIs and other documents) by startups to prove to their investors that those weren't essentially made up last night, providing better assurance of the accuracy of this information.</p> <p>Domain has gone and is now listed on HugeDomains for $2,795</p> <p>streamline.vc was also let go and is now on sale on Dan.com for $7,495</p>"},{"location":"startups/past/#bex-io","title":"Bex.io (2013-2014)","text":"<p>Bex.io was a spin off from Spawngrid's consultancy. In 2013, with all that early hype around Bitcoin, there was a great interest in running exchanges. Some clients approached us to see if they can hire us to build one. </p> <p>Instead, we offered to build Bitcoin exchange as a service (turnkey solution) where clients were to provide local compliance and marketing. </p> <p>We've secured some prospective clients and initial seed funding. Unfortunately, we had our fair share of issues both in leadership and delivery so it was wound down around 2015.</p> <p>Somebody is now trying to sell bex.io domain for $100K</p>"},{"location":"startups/past/#spawngrid","title":"Spawngrid (2011-2013)","text":"<p>Spawngrid started around 2011 as an attempt to build a cloud for deploying complex Erlang-based systems. Over time, overwhelmed with the sheer complexitity of the problem, Spawngrid experimented with some other products like Test.io (automatic test generation product, for which it received some NRC IRAP funding).  Failing to secure clients for this, it became a consultancy company.</p> <p>Domain has gone and is now listed on HugeDomains for $2,195</p>"},{"location":"startups/past/#issuesdone","title":"Issues Done (2007-2008)","text":"<p>An early startup trying to build a web-based Getting Things Done application. Had some fun ideas but wasn't able to market it.</p> <p>The domain is still owned by a friend of mine who was a partner in this venture</p> <p></p>"},{"location":"blog/category/postgres/","title":"Postgres","text":""},{"location":"blog/category/omnigres/","title":"Omnigres","text":""},{"location":"blog/category/d3x/","title":"D3X","text":""},{"location":"blog/category/tips/","title":"tips","text":""},{"location":"blog/category/types/","title":"types","text":""},{"location":"blog/category/rust/","title":"Rust","text":""},{"location":"blog/category/docker/","title":"Docker","text":""},{"location":"blog/category/containers/","title":"containers","text":""},{"location":"blog/category/contest/","title":"contest","text":""},{"location":"blog/category/performance/","title":"performance","text":""},{"location":"blog/category/c/","title":"C","text":""},{"location":"blog/category/git/","title":"Git","text":""},{"location":"blog/category/c4/","title":"C4","text":""},{"location":"blog/category/practices/","title":"practices","text":""},{"location":"blog/category/teamwork/","title":"teamwork","text":""},{"location":"blog/page/2/","title":"Index","text":""}]}